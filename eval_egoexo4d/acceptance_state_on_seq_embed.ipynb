{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a_{i}, s_{i} = f(\\bar{o}_{:i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from LATMOS import *\n",
    "from gen_augment import state_to_embedding_stats, state_to_embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_augment_batch_trace_fast(original_state, original_embed, num_states, embedding_dim, random_sample_per_step=5):\n",
    "    \"\"\"\n",
    "    Generate negative examples from a given trace using efficient batch operations,\n",
    "    sampling embeddings from Gaussian distributions defined by state means and variances.\n",
    "    Args:\n",
    "        original_state: (batch_size, num_steps) tensor\n",
    "        original_embed: (batch_size, num_steps, embedding_dim) tensor\n",
    "        num_states: int\n",
    "        embedding_dim: int\n",
    "        random_sample_per_step: int\n",
    "    Returns:\n",
    "        tuple: (acceptance_traces, embedding_traces)\n",
    "        acceptance_traces: (batch_size * random_sample_per_step * num_steps + batch_size, num_steps) tensor\n",
    "        embedding_traces: (batch_size * random_sample_per_step * num_steps + batch_size, num_steps, embedding_dim) tensor\n",
    "    \"\"\"\n",
    "    batch_size, num_steps = original_state.shape\n",
    "    device = original_state.device\n",
    "    \n",
    "    # Create indices for each step\n",
    "    steps = torch.arange(num_steps, device=device)\n",
    "    # Create mask for acceptance (1s before step, 0s after)\n",
    "    # Shape: (num_steps, num_steps)\n",
    "    acceptance_mask = steps.unsqueeze(0) < steps.unsqueeze(1)\n",
    "    \n",
    "    # Expand acceptance mask for all samples and batches\n",
    "    # Shape: (num_steps, random_sample_per_step, batch_size, num_steps)\n",
    "    acceptance_traces = acceptance_mask.unsqueeze(1).unsqueeze(1).expand(\n",
    "        num_steps, random_sample_per_step, batch_size, num_steps\n",
    "    )\n",
    "    \n",
    "    # Expand original state for copying\n",
    "    # Shape: (num_steps, random_sample_per_step, batch_size, num_steps)\n",
    "    original_expanded = original_state.unsqueeze(0).unsqueeze(1).expand(\n",
    "        num_steps, random_sample_per_step, batch_size, num_steps\n",
    "    )\n",
    "    \n",
    "    # Generate random states\n",
    "    # Shape: (num_steps, random_sample_per_step, batch_size, num_steps)\n",
    "    random_states = torch.randint(0, num_states,\n",
    "        (num_steps, random_sample_per_step, batch_size, num_steps),\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create mask for selecting between original and random states\n",
    "    # Shape: (num_steps, random_sample_per_step, batch_size, num_steps)\n",
    "    selection_mask = acceptance_mask.unsqueeze(1).unsqueeze(1).expand(\n",
    "        num_steps, random_sample_per_step, batch_size, num_steps\n",
    "    )\n",
    "    \n",
    "    # Combine original and random states using the mask\n",
    "    # Shape: (num_steps, random_sample_per_step, batch_size, num_steps)\n",
    "    state_traces = torch.where(\n",
    "        selection_mask,\n",
    "        original_expanded,\n",
    "        random_states\n",
    "    )\n",
    "    \n",
    "    # Reshape state traces\n",
    "    # Shape: (num_steps * random_sample_per_step * batch_size, num_steps)\n",
    "    state_traces = state_traces.reshape(-1, num_steps)\n",
    "    \n",
    "    # Convert states to embeddings using sampling from Gaussian distributions\n",
    "    # First convert state_to_embedding_stats dict to tensors of means and variances\n",
    "    # Shape: (num_states, embedding_dim)\n",
    "    embedding_means = torch.stack([\n",
    "        state_to_embedding_stats[i]['mean']\n",
    "        for i in range(num_states)\n",
    "    ]).to(device)\n",
    "    \n",
    "    # Shape: (num_states, embedding_dim)\n",
    "    embedding_vars = torch.stack([\n",
    "        state_to_embedding_stats[i]['std']\n",
    "        for i in range(num_states)\n",
    "    ]).to(device)\n",
    "    \n",
    "    # Get means and variances for all states\n",
    "    # Shape: (num_steps * random_sample_per_step * batch_size, num_steps, embedding_dim)\n",
    "    trace_means = embedding_means[state_traces]\n",
    "    trace_std = embedding_vars[state_traces]\n",
    "\n",
    "    # Sample from Gaussian distributions\n",
    "    # Using reparameterization trick: z = mu + sigma * epsilon\n",
    "    epsilon = torch.randn_like(trace_means)\n",
    "    embedding_traces = trace_means + trace_std * epsilon\n",
    "    \n",
    "    # Reshape acceptance traces to match embedding_traces\n",
    "    # Shape: (num_steps * random_sample_per_step * batch_size, num_steps)\n",
    "    acceptance_traces = acceptance_traces.reshape(-1, num_steps)\n",
    "    \n",
    "    # Add original positive examples\n",
    "    original_acceptance = torch.ones((batch_size, num_steps), device=device)\n",
    "    acceptance_traces = torch.cat((original_acceptance, acceptance_traces), dim=0)\n",
    "    embedding_traces = torch.cat((original_embed, embedding_traces), dim=0)\n",
    "    state_traces = torch.cat((original_state, state_traces), dim=0)\n",
    "    return acceptance_traces.to(torch.long), embedding_traces, state_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_augment_batch_trace_slow(original_state, original_embed, num_states, embedding_dim, random_sample_per_step=5):\n",
    "    \"\"\"\n",
    "    Generate negative examples from a given trace.\n",
    "    Args:\n",
    "    original_state: (batch_size, num_steps) tensor\n",
    "    num_states: int\n",
    "    Returns:\n",
    "    tuple: (acceptance_traces, state_traces)\n",
    "        acceptance_traces: (batch_size * random_sample_per_step * num_steps + 1, num_steps) tensor\n",
    "        state_traces: (batch_size * random_sample_per_step * num_steps + 1, num_steps) tensor\n",
    "    \"\"\"\n",
    "    batch_size, num_steps = original_state.shape\n",
    "    device = original_state.device\n",
    "\n",
    "    # Initialize tensors to collect traces\n",
    "    acceptance_traces = torch.zeros((num_steps, random_sample_per_step, batch_size, num_steps), device=device)\n",
    "    state_traces = torch.zeros((num_steps, random_sample_per_step, batch_size, num_steps), device=device, dtype=torch.int)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        random_states = torch.randint(0, num_states, (random_sample_per_step, batch_size, num_steps-step), device=device)\n",
    "        acceptance_traces[step, :, :, :step] = 1\n",
    "        state_traces     [step, :, :, :step] = original_state[:, :step]\n",
    "        state_traces     [step, :, :, step:] = random_states\n",
    "\n",
    "    # Reshape tensors\n",
    "    acceptance_traces = acceptance_traces.view(-1, num_steps)\n",
    "    state_traces = state_traces.view(-1, num_steps)\n",
    "\n",
    "    # Concatenate with the original positive trace\n",
    "    original_acceptance = torch.ones((batch_size, num_steps), device=device)\n",
    "    acceptance_traces = torch.cat((original_acceptance, acceptance_traces), dim=0)\n",
    "\n",
    "    embedding_traces = torch.zeros((state_traces.shape[0], state_traces.shape[1], embedding_dim))\n",
    "    for i in range(1, state_traces.shape[0]):\n",
    "        for j in range(state_traces.shape[1]):\n",
    "            embedding_traces[i, j] = random.choice(state_to_embedding_map[state_traces[i, j].item()])\n",
    "    embedding_traces = torch.cat((original_embed, embedding_traces.to(device)), dim=0)\n",
    "\n",
    "    state_traces = torch.cat((original_state, state_traces), dim=0)\n",
    "    return acceptance_traces.to(torch.long), embedding_traces, state_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(data, batch_size):\n",
    "    dataloaders = {}\n",
    "    for num_steps, (embedding_traces, acceptance_traces, state_traces) in data.items():\n",
    "        if num_steps < 1:\n",
    "            continue\n",
    "        dataset = TensorDataset(embedding_traces.to(device), acceptance_traces.to(torch.long).to(device), state_traces.to(torch.long).to(device))\n",
    "        dataloaders[num_steps] = DataLoader(dataset, batch_size, True)\n",
    "    return dataloaders\n",
    "\n",
    "# Load and prepare data\n",
    "random_sample_per_step = 0\n",
    "data_train = torch.load(f'data_augment/segment_train_{random_sample_per_step}.pt')\n",
    "data_val = torch.load(f'data_augment/segment_val_{random_sample_per_step}.pt')\n",
    "\n",
    "# embed_size = 512\n",
    "# embed_size = 1536\n",
    "embed_size = 2304\n",
    "\n",
    "num_states = 629\n",
    "batch_size = 2**12\n",
    "\n",
    "train_loaders = prepare_dataloaders(data_train, batch_size)\n",
    "val_loaders = prepare_dataloaders(data_val, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "def train_model_egoexo4d(model, train_loaders, val_loaders, \n",
    "                         num_epochs, learning_rate, patience):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    num_steps_list = list(train_loaders.keys())\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Metrics recording\n",
    "    train_loss_history = []\n",
    "    train_metrics_history = []\n",
    "    val_metrics_history = []\n",
    "    val_epochs = []\n",
    "\n",
    "    bar = tqdm(range(num_epochs))\n",
    "    for epoch in bar:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        acceptance_TP, acceptance_TN, acceptance_FP, acceptance_FN = 0, 0, 0, 0\n",
    "        state_correct = 0\n",
    "        total_step = 0\n",
    "        \n",
    "        random.shuffle(num_steps_list)\n",
    "        for num_step in num_steps_list:\n",
    "            loader = train_loaders[num_step]\n",
    "            \n",
    "            for batch_embedding, batch_acceptance, batch_state in loader:\n",
    "                batch_acceptance, batch_embedding, batch_state = gen_augment_batch_trace_fast(batch_state, batch_embedding, num_states, \n",
    "                                                                            embed_size, random_sample_per_step=1)\n",
    "                batch_acceptance = batch_acceptance.to(device)\n",
    "                batch_embedding = batch_embedding.to(device)\n",
    "                batch_state = batch_state.to(device)\n",
    "                for _ in range(4):\n",
    "                    optimizer.zero_grad()\n",
    "                    # Forward pass\n",
    "                    state_output, acceptance_output = model(batch_embedding)\n",
    "                    \n",
    "                    # Compute losses\n",
    "                    acceptance_output_flat = acceptance_output.view(-1, 2)\n",
    "                    batch_acceptance_flat = batch_acceptance.view(-1)\n",
    "                    acceptance_loss = criterion(acceptance_output_flat, batch_acceptance_flat)\n",
    "                    \n",
    "                    state_output_flat = state_output.view(-1, num_states)\n",
    "                    batch_state_flat = batch_state.view(-1)\n",
    "                    state_loss = criterion(state_output_flat, batch_state_flat)\n",
    "                    loss = acceptance_loss + state_loss\n",
    "                    \n",
    "                    # loss = state_loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Calculate acceptance accuracy\n",
    "                predicted_acceptance = acceptance_output.argmax(dim=-1).bool()\n",
    "                batch_acceptance = batch_acceptance.bool() if batch_acceptance.dtype != torch.bool else batch_acceptance\n",
    "                acceptance_TP += ( predicted_acceptance &  batch_acceptance).sum().item()\n",
    "                acceptance_TN += (~predicted_acceptance & ~batch_acceptance).sum().item()\n",
    "                acceptance_FP += ( predicted_acceptance & ~batch_acceptance).sum().item()\n",
    "                acceptance_FN += (~predicted_acceptance &  batch_acceptance).sum().item()\n",
    "                # Calculate state accuracy\n",
    "                predicted_states = state_output.argmax(dim=-1)\n",
    "                state_correct += (predicted_states == batch_state).sum().item()\n",
    "\n",
    "                total_step += batch_acceptance.size(0) * batch_acceptance.size(1)\n",
    "\n",
    "                del batch_acceptance, batch_state, batch_embedding\n",
    "            \n",
    "        # Calculate state accuracy \n",
    "        state_accuracy = state_correct / total_step if total_step > 0 else 0\n",
    "\n",
    "        train_loss_history.append(total_loss / total_step)\n",
    "        train_metrics_history.append({\n",
    "            'TP': acceptance_TP / total_step,\n",
    "            'TN': acceptance_TN / total_step,\n",
    "            'FP': acceptance_FP / total_step,\n",
    "            'FN': acceptance_FN / total_step,\n",
    "            'state_accuracy': state_accuracy\n",
    "        })\n",
    "\n",
    "        # Early stopping check\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "\n",
    "        # Validation\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            val_acceptance_TP, val_acceptance_TN, val_acceptance_FP, val_acceptance_FN, val_state_accuracy = evaluate_model_egoexo4d(model, val_loaders)\n",
    "            val_epochs.append(epoch + 1)\n",
    "            val_metrics_history.append({\n",
    "                'TP': val_acceptance_TP,\n",
    "                'TN': val_acceptance_TN,\n",
    "                'FP': val_acceptance_FP,\n",
    "                'FN': val_acceptance_FN,\n",
    "                'state_accuracy': val_state_accuracy\n",
    "            })\n",
    "            print({\n",
    "                'TP': val_acceptance_TP,\n",
    "                'TN': val_acceptance_TN,\n",
    "                'FP': val_acceptance_FP,\n",
    "                'FN': val_acceptance_FN,\n",
    "                'state_accuracy': val_state_accuracy\n",
    "            })\n",
    "\n",
    "        bar.set_postfix({\"Average Loss\": total_loss / total_step, \n",
    "                         'train_acceptance_TP': acceptance_TP / total_step,\n",
    "                         'train_acceptance_TN': acceptance_TN / total_step,\n",
    "                         'train_acceptance_FP': acceptance_FP / total_step,\n",
    "                         'train_acceptance_FN': acceptance_FN / total_step,\n",
    "                         'train_state_accuracy': state_accuracy})\n",
    "\n",
    "    return train_loss_history, train_metrics_history, val_epochs, val_metrics_history\n",
    "\n",
    "def evaluate_model_egoexo4d(model, val_loaders):\n",
    "    model.eval()\n",
    "    acceptance_TP, acceptance_TN, acceptance_FP, acceptance_FN = 0, 0, 0, 0\n",
    "    state_correct = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, loader in val_loaders.items():\n",
    "            for batch_embedding, batch_acceptance, batch_state in loader:\n",
    "                batch_acceptance, batch_embedding, batch_state = gen_augment_batch_trace_slow(batch_state, batch_embedding, num_states, embed_size, random_sample_per_step=1)\n",
    "                batch_acceptance = batch_acceptance.to(device)\n",
    "                batch_embedding = batch_embedding.to(device)\n",
    "                batch_state = batch_state.to(device)\n",
    "\n",
    "                state_output, acceptance_output = model(batch_embedding)\n",
    "                \n",
    "                # Calculate acceptance accuracy\n",
    "                predicted_acceptance = acceptance_output.argmax(dim=-1).bool()\n",
    "                batch_acceptance = batch_acceptance.bool() if batch_acceptance.dtype != torch.bool else batch_acceptance\n",
    "                acceptance_TP += ( predicted_acceptance &  batch_acceptance).sum().item()\n",
    "                acceptance_TN += (~predicted_acceptance & ~batch_acceptance).sum().item()\n",
    "                acceptance_FP += ( predicted_acceptance & ~batch_acceptance).sum().item()\n",
    "                acceptance_FN += (~predicted_acceptance &  batch_acceptance).sum().item()\n",
    "\n",
    "                # Calculate state accuracy\n",
    "                _, predicted_states = torch.max(state_output, -1)\n",
    "                state_correct += (predicted_states == batch_state).sum().item()\n",
    "                \n",
    "                total_steps += batch_acceptance.size(0) * batch_acceptance.size(1)\n",
    "\n",
    "                del batch_acceptance, batch_state\n",
    "\n",
    "    state_accuracy = state_correct / total_steps if total_steps > 0 else 0\n",
    "    \n",
    "    return (acceptance_TP/total_steps, \n",
    "            acceptance_TN/total_steps, \n",
    "            acceptance_FP/total_steps, \n",
    "            acceptance_FN/total_steps,\n",
    "            state_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = create_model('attention',\n",
    "                     input_size  = embed_size, \n",
    "                     hidden_size = embed_size, \n",
    "                     output_size = num_states, \n",
    "                     device=device)\n",
    "model.get_model_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "losses = train_model_egoexo4d(model, train_loaders, val_loaders, \n",
    "                     num_epochs=80, learning_rate=1e-5,\n",
    "                     patience=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

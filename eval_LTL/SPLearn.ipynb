{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from splearn.spectral import Spectral\n",
    "from splearn.datasets.data_sample import SplearnArray\n",
    "from splearn.datasets.base import load_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_data(idx, train_var, val_var):\n",
    "    \"\"\"\n",
    "    Load and prepare training and validation data.\n",
    "    \n",
    "    Args:\n",
    "        idx (int): Index parameter for data file\n",
    "        train_var (int): Training variation parameter\n",
    "        val_var (int): Validation variation parameter\n",
    "        \n",
    "    Returns:\n",
    "        tuple: train_data_sp, val_data_sp, num_states\n",
    "    \"\"\"\n",
    "    data = np.load(f'data/spot_{idx}_{train_var}_{val_var}.npz')\n",
    "    train_state_vectors = data['train_state_vectors']\n",
    "    train_ap_vectors = data['train_ap_vectors']\n",
    "    train_acceptance_vectors = data['train_acceptance_vectors']\n",
    "    val_state_vectors = data['val_state_vectors']\n",
    "    val_ap_vectors = data['val_ap_vectors']\n",
    "    val_acceptance_vectors = data['val_acceptance_vectors']\n",
    "\n",
    "    _, seq_len, ap_dim = train_ap_vectors.shape\n",
    "\n",
    "    powers_of_2 = 2 ** np.arange(ap_dim)\n",
    "\n",
    "    train_data = np.ones((train_acceptance_vectors[:, 1:].sum(), seq_len+1)) * -1\n",
    "    data_idx = 0\n",
    "    for ap, acceptance in zip(train_ap_vectors, train_acceptance_vectors):\n",
    "        ap_binary_coded = np.dot(ap, powers_of_2)\n",
    "        for step in np.where(acceptance)[0]:\n",
    "            train_data[data_idx, :step] = ap_binary_coded[:step]\n",
    "            data_idx += 1\n",
    "    train_data_sp = SplearnArray(train_data, nbL=int(2**ap_dim), nbEx=train_data.shape[0])\n",
    "\n",
    "    val_data = np.ones((val_acceptance_vectors[:, 1:].shape[0] * val_acceptance_vectors[:, 1:].shape[1], seq_len)) * -1\n",
    "    val_y = np.zeros(val_acceptance_vectors[:, 1:].shape[0] * val_acceptance_vectors[:, 1:].shape[1])\n",
    "    data_idx = 0\n",
    "    for ap, acceptance in zip(val_ap_vectors, val_acceptance_vectors):\n",
    "        ap_binary_coded = np.dot(ap, powers_of_2)\n",
    "        for step in range(1,len(acceptance)):\n",
    "            val_data[data_idx, :step] = ap_binary_coded[:step]\n",
    "            val_y[data_idx] = acceptance[step]\n",
    "            data_idx += 1\n",
    "    val_data_sp = SplearnArray(val_data, nbL=int(2**ap_dim), nbEx=val_data.shape[0])\n",
    "    \n",
    "    # train_filename = f'SP_{idx}_{train_var}_{val_var}_train.txt'\n",
    "    # with open(train_filename, 'w') as f:\n",
    "    #     data_idx = 0\n",
    "    #     f.write(f'{train_acceptance_vectors[:, 1:].sum()} {2**ap_dim}\\n')\n",
    "    #     for ap, acceptance in tqdm(zip(train_ap_vectors, train_acceptance_vectors)):\n",
    "    #         ap_binary_coded = np.dot(ap, powers_of_2)\n",
    "    #         for step in np.where(acceptance)[0]:\n",
    "    #             ap_str = ' '.join(map(str, ap_binary_coded[:step]))\n",
    "    #             f.write(f'{step}_{ap_str}\\n')\n",
    "    # train = load_data_sample(train_filename)\n",
    "\n",
    "    # val_filename = f'SP_{idx}_{train_var}_{val_var}_val.txt'\n",
    "    # with open(val_filename, 'w') as f:\n",
    "    #     data_idx = 0\n",
    "    #     f.write(f'{val_acceptance_vectors[:, 1:].shape[0] * val_acceptance_vectors[:, 1:].shape[1]} {2**ap_dim}\\n')\n",
    "    #     for ap, acceptance in tqdm(zip(val_ap_vectors, val_acceptance_vectors)):\n",
    "    #         ap_binary_coded = np.dot(ap, powers_of_2)\n",
    "    #         for step in range(1,len(acceptance)):\n",
    "    #             ap_str = ' '.join(map(str, ap_binary_coded[:step]))\n",
    "    #             f.write(f'{step}_{ap_str}\\n')\n",
    "    # val = load_data_sample(val_filename)\n",
    "    return train_data_sp, val_data_sp, val_y, train_state_vectors.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_classifier(y_true, y_pred_prob):\n",
    "    \"\"\"\n",
    "    Find optimal threshold for best accuracy and calculate AUC.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth labels (0 or 1)\n",
    "        y_pred_prob: Predicted probabilities or scores\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_threshold, best_accuracy, roc_auc)\n",
    "    \"\"\"\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Find threshold that maximizes accuracy\n",
    "    best_accuracy = 0\n",
    "    best_threshold = 0\n",
    "    \n",
    "    # Loop through potential thresholds to find best accuracy\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_accuracy, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(idx, train_var, val_var, HIDDEN_FACTOR):\n",
    "    train_data_sp, val_data_sp, val_y, num_states = load_and_prep_data(idx, train_var, val_var)\n",
    "    # print(\"*\", val_y.sum()/len(val_y))\n",
    "    est = Spectral()\n",
    "    est.set_params(partial=True, rank = int(num_states*HIDDEN_FACTOR))\n",
    "    # est.get_params()\n",
    "    est.fit(train_data_sp)\n",
    "    val_prediction = est.predict(val_data_sp)\n",
    "    _, accuracy, _ = evaluate_binary_classifier(val_y, val_prediction)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars = [0, 0, 0, 0.1, 0.2]\n",
    "val_vars = [0, 0.1, 0.2, 0, 0]\n",
    "# train_vars = [0]\n",
    "# val_vars = [0]\n",
    "indices = [0, 1, 2]\n",
    "HIDDEN_FACTOR = 12\n",
    "\n",
    "a = []\n",
    "for hf in [0.75]:\n",
    "    re = []\n",
    "    for idx in [0, 1, 2]:\n",
    "        re.append(test(idx, 0, 0, hf))\n",
    "    a.append(re)\n",
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.mean(a,axis=1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(3, 0, 0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

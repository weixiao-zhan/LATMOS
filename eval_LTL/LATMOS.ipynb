{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from LATMOS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_spot(model, train_loader, val_loader, \n",
    "                num_epochs, learning_rate,  patience=30):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    bar = tqdm(range(num_epochs), leave=False)\n",
    "    for epoch in bar:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_batches = 0\n",
    "\n",
    "        for batch_ap, batch_state, batch_acceptance in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            state_output, acceptance_output = model(batch_ap)\n",
    "\n",
    "            # Flatten acceptance output and target\n",
    "            state_output_flat = state_output.reshape(-1, state_output.shape[-1])\n",
    "            batch_state_flat = batch_state[:, 1:].reshape(-1)\n",
    "            acceptance_output_flat = acceptance_output.reshape(-1, 2)\n",
    "            batch_acceptance_flat = batch_acceptance[:, 1:].reshape(-1)\n",
    "            # Compute losses\n",
    "            state_loss = criterion(state_output_flat, batch_state_flat)\n",
    "            acceptance_loss = criterion(acceptance_output_flat, batch_acceptance_flat)\n",
    "            loss = state_loss + acceptance_loss\n",
    "            # loss = acceptance_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.detach().item()\n",
    "            total_batches +=  batch_ap.shape[0]\n",
    "\n",
    "        avg_loss = total_loss / total_batches\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "\n",
    "        # Evaluate on test set\n",
    "        train_accuracy = evaluate_model_spot(model, train_loader)\n",
    "        val_accuracy = evaluate_model_spot(model, val_loader)\n",
    "        bar.set_postfix({\"Average Loss\": avg_loss, \n",
    "                         \"Train State Accuracy\": train_accuracy[0], \"Train Acceptance Accuracy\": train_accuracy[1],\n",
    "                         \"Val State Accuracy\": val_accuracy[0], \"Val Acceptance Accuracy\": val_accuracy[1]})\n",
    "\n",
    "def evaluate_model_spot(model, val_loader):\n",
    "    model.eval()\n",
    "    correct_states = 0\n",
    "    correct_acceptance = 0\n",
    "    total_traces = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_ap, batch_state, batch_acceptance in val_loader:\n",
    "            state_output, acceptance_output = model(batch_ap)\n",
    "            \n",
    "            # For state_output of shape (batch, seq_len, num_states)\n",
    "            predicted_states = torch.argmax(state_output, dim=-1)  # (batch, seq_len)\n",
    "            ground_truth_states = batch_state[:, 1:]               # (batch, seq_len)\n",
    "            \n",
    "            # For acceptance_output of shape (batch, seq_len, 2)\n",
    "            predicted_acceptance = torch.argmax(acceptance_output, dim=-1)  # (batch, seq_len)\n",
    "            ground_truth_acceptance = batch_acceptance[:, 1:]  # Assuming this is already class indices\n",
    "            \n",
    "            # Count sequences where all predictions are correct\n",
    "            correct_states += torch.all(predicted_states == ground_truth_states, dim=1).sum().item()\n",
    "            correct_acceptance += torch.all(predicted_acceptance == ground_truth_acceptance, dim=1).sum().item()\n",
    "            \n",
    "            total_traces += batch_state.size(0)\n",
    "    \n",
    "    return correct_states / total_traces, correct_acceptance / total_traces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "train one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data = np.load(f'data/spot_{idx}_0_0.npz')\n",
    "train_state_vectors = data['train_state_vectors']\n",
    "train_ap_vectors = data['train_ap_vectors']\n",
    "train_acceptance_vectors = data['train_acceptance_vectors']\n",
    "val_state_vectors = data['val_state_vectors']\n",
    "val_ap_vectors = data['val_ap_vectors']\n",
    "val_acceptance_vectors = data['val_acceptance_vectors']\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_ap_tensor = torch.tensor(train_ap_vectors, dtype=torch.float).to(device)\n",
    "train_state_tensor = torch.tensor(train_state_vectors, dtype=torch.long).to(device)\n",
    "train_acceptance_tensor = torch.tensor(train_acceptance_vectors, dtype=torch.long).to(device)\n",
    "\n",
    "val_ap_tensor = torch.tensor(val_ap_vectors, dtype=torch.float).to(device)\n",
    "val_state_tensor = torch.tensor(val_state_vectors, dtype=torch.long).to(device)\n",
    "val_acceptance_tensor = torch.tensor(val_acceptance_vectors, dtype=torch.long).to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_ap_tensor, train_state_tensor, train_acceptance_tensor)\n",
    "val_dataset = TensorDataset(val_ap_tensor, val_state_tensor, val_acceptance_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 2**9\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('ssm', \n",
    "                     input_size=train_ap_tensor.shape[-1], \n",
    "                     hidden_size = train_state_tensor.shape[-1]*8, \n",
    "                     output_size=train_state_tensor.shape[-1], \n",
    "                     device=device)\n",
    "\n",
    "# Train the model\n",
    "train_model_spot(model, train_loader, val_loader, num_epochs=1000, learning_rate=1e-3)\n",
    "\n",
    "# Final evaluation\n",
    "final_accuracy = evaluate_model_spot(model, val_loader)\n",
    "print(f\"Final Test Accuracy: {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "report on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report():\n",
    "    # Define dimensions\n",
    "    models = ['gru', 'attention', 'ssm']  # Currently only one model\n",
    "    train_vars = [0, 0, 0, 0.1, 0.2]\n",
    "    val_vars = [0, 0.1, 0.2, 0, 0]\n",
    "    train_vars = [0]\n",
    "    val_vars = [0]\n",
    "    indices = [0, 1, 2]\n",
    "    HIDDEN_FACTOR = 12\n",
    "\n",
    "    # Initialize results arrays\n",
    "    # Shape: (num_models, num_variance_pairs, num_indices)\n",
    "    train_accuracies = np.zeros((len(models), len(train_vars), len(indices), 2))\n",
    "    val_accuracies = np.zeros_like(train_accuracies)\n",
    "    \n",
    "    # Nested loops with explicit indices for array assignment\n",
    "    for model_idx, nn_model in enumerate(models):\n",
    "        for var_idx, (train_var, val_var) in enumerate(zip(train_vars, val_vars)):\n",
    "            for data_idx, idx in enumerate(indices):\n",
    "                \n",
    "                data = np.load(f'data/spot_{idx}_{train_var}_{val_var}.npz')\n",
    "                \n",
    "                # Load and process data\n",
    "                train_state_vectors = data['train_state_vectors']\n",
    "                train_ap_vectors = data['train_ap_vectors']\n",
    "                train_acceptance_vectors = data['train_acceptance_vectors']\n",
    "                val_state_vectors = data['val_state_vectors']\n",
    "                val_ap_vectors = data['val_ap_vectors']\n",
    "                val_acceptance_vectors = data['val_acceptance_vectors']\n",
    "                \n",
    "                # Convert to PyTorch tensors\n",
    "                train_ap_tensor = torch.tensor(train_ap_vectors, dtype=torch.float).to(device)\n",
    "                train_state_tensor = torch.tensor(train_state_vectors, dtype=torch.long).to(device)\n",
    "                train_acceptance_tensor = torch.tensor(train_acceptance_vectors, dtype=torch.long).to(device)\n",
    "                val_ap_tensor = torch.tensor(val_ap_vectors, dtype=torch.float).to(device)\n",
    "                val_state_tensor = torch.tensor(val_state_vectors, dtype=torch.long).to(device)\n",
    "                val_acceptance_tensor = torch.tensor(val_acceptance_vectors, dtype=torch.long).to(device)\n",
    "                \n",
    "                # Create datasets and dataloaders\n",
    "                train_dataset = TensorDataset(train_ap_tensor, train_state_tensor, train_acceptance_tensor)\n",
    "                val_dataset = TensorDataset(val_ap_tensor, val_state_tensor, val_acceptance_tensor)\n",
    "                \n",
    "                batch_size = 2**9\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                # Create and train model\n",
    "                model = create_model(nn_model,\n",
    "                                  input_size=train_ap_tensor.shape[-1],\n",
    "                                  hidden_size=int(train_state_tensor.shape[-1]*HIDDEN_FACTOR),\n",
    "                                  output_size=train_state_tensor.shape[-1],\n",
    "                                  device=device)\n",
    "                \n",
    "                train_model_spot(model, train_loader, val_loader, num_epochs=500, learning_rate=1e-4)\n",
    "                \n",
    "                # Evaluate and store results\n",
    "                train_acc = evaluate_model_spot(model, train_loader)\n",
    "                val_acc = evaluate_model_spot(model, val_loader)\n",
    "                \n",
    "                train_accuracies[model_idx, var_idx, data_idx] = train_acc\n",
    "                val_accuracies[model_idx, var_idx, data_idx] = val_acc\n",
    "                \n",
    "                # Print current results\n",
    "                print(f\"{nn_model:5} on {idx:2}\\n\"\n",
    "                      f\"| train ({train_var}): {train_acc}\\n\"\n",
    "                      f\"| val ({val_var}): {val_acc}\")\n",
    "                \n",
    "                # Cleanup\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return train_accuracies, val_accuracies\n",
    "\n",
    "# Example usage\n",
    "train_accs, val_accs = report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can analyze the results using numpy operations\n",
    "# For example, to get mean accuracies across datasets for each variance pair:\n",
    "mean_train_accs = np.mean(train_accs, axis=2)  # Average across datasets\n",
    "mean_val_accs = np.mean(val_accs, axis=2)\n",
    "\n",
    "mean_val_accs.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
